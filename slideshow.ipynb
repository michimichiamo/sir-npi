{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A descriptive optimization model for epidemic scenarios (SARS-CoV-2) exploiting the SIR compartmental model and Lagrangian Duality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ## Imports\n",
    "\n",
    "# Data management and manipulation\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "import datetime # dates\n",
    "from util.SIRModel.reader import download_data, read_SIR\n",
    "\n",
    "# Model\n",
    "from util.SIRModel.model import SIRModel, save_parameters\n",
    "\n",
    "# Plotting\n",
    "from util.SIRModel.plotter import plotter, plot_loss_history, plot_pars, plot_SIR#, plot_prediction, plot_RK\n",
    "\n",
    "from tqdm import tqdm # nice progress bars\n",
    "\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "repository = 'https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/'\n",
    "url = 'dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv'\n",
    "\n",
    "pop_aug = 59392408 # Italy population at 1.08.2020\n",
    "pop_sep = 59376037 # Italy population at 1.09.2020\n",
    "N = round((pop_aug+pop_sep)/2) # Italy population estimate at 15.08.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Outline <br/><br/>\n",
    "\n",
    "\n",
    "* Goal\n",
    "* Modelling the problem\n",
    "    - Available data\n",
    "    - SIR epidemiological model\n",
    "    - Lagrangian Duality for Constrained Machine Learning\n",
    "    - Implementation\n",
    "* Data preprocessing\n",
    "* Model definition\n",
    "* Training\n",
    "* Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal <br/><br/>\n",
    "\n",
    "- The aim of the present work is to provide a **descriptive model for epidemic scenarios**\n",
    "- In particular, we want to train an *optimization algorithm* on currently available data so as to\n",
    "  - **estimate the <b>parameters</b> of the epidemic**\n",
    "  - **reproduce the <b>evolution</b> of the epidemic**, with focus on the infected individuals.\n",
    "- We exploit a **hybrid** approach, combining **model-driven** and **data-driven** techniques, as we will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelling the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Available data<br/><br/>\n",
    "\n",
    "First of all, we need to figure out how available data is structured, in order to design our model.\n",
    "\n",
    "- We rely on the official [COVID-19](https://github.com/pcm-dpc/COVID-19) data repository — provided by the Presidenza del Consiglio dei Ministri (specifically, by the Protezione Civile department)\n",
    "  - it is organized in different sections according to different aggregation levels: national, regional, provincial.\n",
    "  - Each section consists of `.csv` files, one for each day, containing tabular data: in particular, we focused on national data, thus inspecting the `dati-andamento-nazionale` section.\n",
    "  - The most interesting columns of these files are `['totale_positivi', 'dimessi_guariti', 'deceduti']`, since they represent the core information about the pandemic: the number of infected, of healed, and of deceased people.\n",
    "- As we see in what follows, this data allows us to build a model to describe the trend of the epidemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting data<br/><br/>\n",
    "\n",
    "- Let's download raw data from the correspondent [repository](https://github.com/pcm-dpc/COVID-19/tree/master/dati-andamento-nazionale) and import it into a `pandas.DataFrame` object to inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>stato</th>\n",
       "      <th>ricoverati_con_sintomi</th>\n",
       "      <th>terapia_intensiva</th>\n",
       "      <th>totale_ospedalizzati</th>\n",
       "      <th>isolamento_domiciliare</th>\n",
       "      <th>totale_positivi</th>\n",
       "      <th>variazione_totale_positivi</th>\n",
       "      <th>nuovi_positivi</th>\n",
       "      <th>dimessi_guariti</th>\n",
       "      <th>...</th>\n",
       "      <th>tamponi</th>\n",
       "      <th>casi_testati</th>\n",
       "      <th>note</th>\n",
       "      <th>ingressi_terapia_intensiva</th>\n",
       "      <th>note_test</th>\n",
       "      <th>note_casi</th>\n",
       "      <th>totale_positivi_test_molecolare</th>\n",
       "      <th>totale_positivi_test_antigenico_rapido</th>\n",
       "      <th>tamponi_test_molecolare</th>\n",
       "      <th>tamponi_test_antigenico_rapido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-24T18:00:00</td>\n",
       "      <td>ITA</td>\n",
       "      <td>101</td>\n",
       "      <td>26</td>\n",
       "      <td>127</td>\n",
       "      <td>94</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-25T18:00:00</td>\n",
       "      <td>ITA</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>150</td>\n",
       "      <td>162</td>\n",
       "      <td>311</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-26T18:00:00</td>\n",
       "      <td>ITA</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "      <td>164</td>\n",
       "      <td>221</td>\n",
       "      <td>385</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-27T18:00:00</td>\n",
       "      <td>ITA</td>\n",
       "      <td>248</td>\n",
       "      <td>56</td>\n",
       "      <td>304</td>\n",
       "      <td>284</td>\n",
       "      <td>588</td>\n",
       "      <td>203</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>12014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-28T18:00:00</td>\n",
       "      <td>ITA</td>\n",
       "      <td>345</td>\n",
       "      <td>64</td>\n",
       "      <td>409</td>\n",
       "      <td>412</td>\n",
       "      <td>821</td>\n",
       "      <td>233</td>\n",
       "      <td>238</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>15695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  data stato  ricoverati_con_sintomi  terapia_intensiva  \\\n",
       "0  2020-02-24T18:00:00   ITA                     101                 26   \n",
       "1  2020-02-25T18:00:00   ITA                     114                 35   \n",
       "2  2020-02-26T18:00:00   ITA                     128                 36   \n",
       "3  2020-02-27T18:00:00   ITA                     248                 56   \n",
       "4  2020-02-28T18:00:00   ITA                     345                 64   \n",
       "\n",
       "   totale_ospedalizzati  isolamento_domiciliare  totale_positivi  \\\n",
       "0                   127                      94              221   \n",
       "1                   150                     162              311   \n",
       "2                   164                     221              385   \n",
       "3                   304                     284              588   \n",
       "4                   409                     412              821   \n",
       "\n",
       "   variazione_totale_positivi  nuovi_positivi  dimessi_guariti  ...  tamponi  \\\n",
       "0                           0             221                1  ...     4324   \n",
       "1                          90              93                1  ...     8623   \n",
       "2                          74              78                3  ...     9587   \n",
       "3                         203             250               45  ...    12014   \n",
       "4                         233             238               46  ...    15695   \n",
       "\n",
       "   casi_testati  note  ingressi_terapia_intensiva  note_test  note_casi  \\\n",
       "0           NaN   NaN                         NaN        NaN        NaN   \n",
       "1           NaN   NaN                         NaN        NaN        NaN   \n",
       "2           NaN   NaN                         NaN        NaN        NaN   \n",
       "3           NaN   NaN                         NaN        NaN        NaN   \n",
       "4           NaN   NaN                         NaN        NaN        NaN   \n",
       "\n",
       "  totale_positivi_test_molecolare  totale_positivi_test_antigenico_rapido  \\\n",
       "0                             NaN                                     NaN   \n",
       "1                             NaN                                     NaN   \n",
       "2                             NaN                                     NaN   \n",
       "3                             NaN                                     NaN   \n",
       "4                             NaN                                     NaN   \n",
       "\n",
       "   tamponi_test_molecolare  tamponi_test_antigenico_rapido  \n",
       "0                      NaN                             NaN  \n",
       "1                      NaN                             NaN  \n",
       "2                      NaN                             NaN  \n",
       "3                      NaN                             NaN  \n",
       "4                      NaN                             NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(repository+url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The SIR epidemiological model<br/><br/>\n",
    "\n",
    "The simplest *compartmental model* to describe the spread of infectious diseases is the **SIR model** without vital dynamics — i.e. in which birth and death rates are neglected — which can be expressed through the following set of ordinary differential equations (ODEs):\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "{\\displaystyle \\frac{dS}{dt} = \\frac{-\\beta I S}{N}} \\\\\n",
    "{\\displaystyle \\frac{dI}{dt} = \\frac{\\beta I S}{N} - \\gamma I} \\\\\n",
    "{\\displaystyle \\frac{dR}{dt} = \\gamma I} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $N$ is the total population\n",
    "- $S$ is the *compartment* of **S**usceptible (i.e. healthy), $I$ of **I**nfected (diseased and infectious) and $R$ of **R**ecovered (either healed and immunized or deceased) individuals\n",
    "  - Note that $S$, $I$ and $R$ are mutually exclusive, so that $N=S+I+R$\n",
    "- $\\beta>0$ is the <b>infection rate</b><sup>1</sup>, and $0<\\gamma<1$ is the <b>recovery rate</b><sup>2</sup>.\n",
    "\n",
    "<br><p class=\"tiny\"><sup>1</sup>Defined as the <em>average number of contacts</em> per person per time, multiplied by the <em>probability of disease transmission</em> in a contact between a susceptible and an infectious subject.<br>\n",
    "<sup>2</sup>Defined as the inverse of the degency period, i.e. the time that an individual stays in the $I$ compartment (before moving to $R$).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The SIR epidemiological model<br/><br/>\n",
    "\n",
    "- (Almost) everything we need is stored in the data:\n",
    "  - $I$ compartment data corresponds to column `'totale_positivi'`\n",
    "  - $R$ data can be computed as the sum of the columns `'dimessi_guariti', 'deceduti'`\n",
    "  - Recalling that $N = S + I + R$, we collect the total population $N$ from [Istat](http://demo.istat.it/bilmens/index.php?anno=2020&lingua=ita) data and compute $S$ as $N - I - R$\n",
    "- We can now tackle the problem — as mentioned before — using a *hybrid* approach, consisting of:\n",
    "  * A **model-driven** approach: the set of ODEs above can be numerically solved through iterative methods such as the [Runge-Kutta methods](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) (in particular, the first-order one, also known as [Euler method](https://en.wikipedia.org/wiki/Euler_method), and the fourth-order one, also referred to as [RK4](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Derivation_of_the_Runge%E2%80%93Kutta_fourth-order_method). Indeed, they allow to numerically compute the evolution of the variables $S(t)$, $I(t)$, $R(t)$, given the initial state $S(0)$, $I(0)$, $R(0)$ and the parameters $\\beta$, $\\gamma$.\n",
    "  * A **data-driven** approach: conversely — given the evolution of $S(t)$, $I(t)$ and $R(t)$ as both data and predictions — we could train an *optimization algorithm* to estimate the best fitting parameters $\\beta$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The SIR epidemiological model<br/><br/>\n",
    "\n",
    "- We can now observe that $\\gamma$ tends to keep a *roughly constant* value during an epidemic, whereas $\\beta$ does not:\n",
    "  * the <b>spread rate</b> of the disease is mainly influenced by continuously floating variables — such as **containment measures** and ultimately **people behaviour**\n",
    "  * the <b>degency period</b>, on the other hand, is affected only by clinical and treatment issues, which are much less prone to variations.\n",
    "- Hence, we can simplify our problem assigning a **constant value to $\\gamma$**<sup>3</sup> and performing the optimization **only over $\\beta$**.\n",
    "- Conversely, we express the latter as a time-dependent variable $\\beta\\left(t\\right)$ so as to capture its significant variability\n",
    "  - Thus, we will more appropriately refer to it as a vector of parameters $\\boldsymbol{\\beta}=(\\beta_{0},...,\\beta_{T-1})$ \n",
    "  - one for each day $t$ with $\\beta_{t}\\equiv\\beta\\left(t\\right)$\n",
    "  - with $T$ the total number of days covered by available data.\n",
    "\n",
    "<br><p class=\"tiny\"><sup>3</sup>We used the value $\\gamma = 1/17$, which corresponds to a degency period of 17 days, obtained by global trends.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The SIR epidemiological model<br/><br/>\n",
    "\n",
    "* The characteristic evolution of a SIR model without vital dynamics\n",
    "\n",
    "<center><div style=\"background-image:url('assets/SIR_model.png'); width:600px; height:300px; background-position:center; background-repeat: no-repeat;\"></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting data<br/><br/>\n",
    "\n",
    "* Let's take a look to data we are interested in\n",
    "* We can notice similarities with the SIR model:\n",
    "  - the `'totale_positivi'` resembles the $I$ compartment\n",
    "  - the `'dimessi_guariti'` resembles the $R$ compartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66315acdf6446119ac23ada1f562d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['totale_positivi', 'dimessi_guariti', 'deceduti']\n",
    "plotter(df[labels].values.T, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data preprocessing<br/><br/>\n",
    "\n",
    "* Data is already pretty clean\n",
    "  * We need to filter the dataframe to keep only useful information\n",
    "  * For convenience, we parse dates setting a `pd.DatetimeIndex`\n",
    "\n",
    "```python\n",
    "def download_data(url, ...):\n",
    "    ...\n",
    "    dataframe = pd.read_csv(url)\n",
    "    ...\n",
    "    # Select interesting columns\n",
    "    dataframe = dataframe.filter(['data', 'totale_positivi', 'dimessi_guariti', 'deceduti'])\n",
    "    # Parse dates\n",
    "    dataframe['data'] = pd.to_datetime(dataframe['data'], format='%Y-%m-%d').dt.date\n",
    "    # Set DatetimeIndex\n",
    "    dataframe = dataframe.set_index(pd.DatetimeIndex(dataframe['data'])).drop('data', axis=1)\n",
    "    return dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dataframe = download_data(repository+url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Retrieve input data<br/><br/>\n",
    "\n",
    "We now select a time interval and extract the corresponding S,I,R variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "start_date=datetime.date(2020, 8, 15)\n",
    "end_date=datetime.date(2021, 5, 1)\n",
    "\n",
    "# Read values\n",
    "S, I, R = read_SIR(dataframe, start_date, end_date, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Retrieve input data\n",
    "\n",
    "```python\n",
    "# Retrieve S,I,R from Dataframe and eventually add padding\n",
    "def read_SIR(dataframe, start_date, end_date, N, ...)\n",
    "    # Select dates\n",
    "    dataframe = dataframe.loc[start_date:end_date]\n",
    "    # Retrieve S, I, R from DataFrame\n",
    "    I = np.array(dataframe['totale_positivi'], dtype='float64')\n",
    "    recovered = np.array(dataframe['dimessi_guariti'], dtype='float64')\n",
    "    deceased = np.array(dataframe['deceduti'], dtype='float64')\n",
    "    R = recovered + deceased\n",
    "    S = N - I - R\n",
    "    ...\n",
    "    return S, I, R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline definition <br/><br/>\n",
    "\n",
    "* First of all, we define a simple **baseline model** to approach the problem\n",
    "* We start by defining a **loss function**:\n",
    "$$\n",
    "f_{\\boldsymbol{\\beta}}\\left(I,R\\right) = \\frac{1}{T}\\sum_{t=0}^{T-1}{\\left(\\hat{I}_{t}-I_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2} + \\epsilon_{R}\\left(\\hat{R}_{t}-R_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2}}\n",
    "$$\n",
    "- Basically, the sum of the **Mean Squared Errors** on both the $I$ and the $R$ compartments, this latter down-weighted by a constant $\\epsilon_{R} < 1$\n",
    "  * both terms depend only on $\\boldsymbol{\\beta}$, which indeed is our only trainable parameter.\n",
    "- Our goal is obtaining the **parameters $\\boldsymbol{\\beta}$** that **best fit the available data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline definition<br/><br/>\n",
    "\n",
    "We come now to the actual model definition, wrapped inside the `SIRModel` class:\n",
    "\n",
    "```python\n",
    "class SIRModel():\n",
    "    # Model creation\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "    # Approximation\n",
    "    def run_simulation(self):\n",
    "        ...\n",
    "    # Train: minimize loss\n",
    "    def train(self, ...):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline definition\n",
    "```python\n",
    "def __init__(self, S, I, R, N, gamma=1./17, eps_R=1e-3):\n",
    "    ...\n",
    "    # Parameters\n",
    "    self.initializer_beta = tf.keras.initializers.RandomUniform(minval=0, seed=42)\n",
    "    self.betas = tf.Variable(self.initializer_beta(shape=(self.T,), dtype='float64'), trainable=True)\n",
    "    # Weight\n",
    "    self.eps_R = tf.constant(eps_R, shape=(), dtype='float64')\n",
    "    # Optimizer\n",
    "    self.optimizer = tf.keras.optimizers.Adam()\n",
    "    # Loss history\n",
    "    self.loss_history = tf.Variable([], dtype='float64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline definition\n",
    "```python\n",
    "def __step(self, t):\n",
    "    S_to_I = (self.betas[t] * self.I_[t] * self.S_[t]) / self.N\n",
    "    I_to_R = self.gammas[t] * self.I_[t]\n",
    "    self.S_ = tf.concat([self.S_, [self.S_[t] - S_to_I]], 0)\n",
    "    self.I_ = tf.concat([self.I_, [self.I_[t] + S_to_I - I_to_R]], 0)\n",
    "    self.R_ = tf.concat([self.R_, [self.R_[t] + I_to_R]], 0)\n",
    "def run_simulation(self):\n",
    "    self.S_ = tf.Variable([self.S[0]], dtype='float64')\n",
    "    self.I_ = tf.Variable([self.I[0]], dtype='float64')\n",
    "    self.R_ = tf.Variable([self.R[0]], dtype='float64')\n",
    "    for t in range(self.T-1):\n",
    "        self.__step(t)\n",
    "```\n",
    "* The *Euler's method* is used to compute the evolution of $S(t)$, $I(t)$ and $R(t)$, given the initial values $S(0)$, $I(0)$ and $R(0)$ along with the parameters $\\boldsymbol{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline definition\n",
    "```python\n",
    "def loss_fn(self):\n",
    "    self.run_simulation()\n",
    "    # Error on I\n",
    "    err_I = K.mean(K.square(self.I_ - self.I))\n",
    "    # Error on R\n",
    "    err_R = self.eps_R * K.mean(K.square(self.R_ - self.R))\n",
    "    loss = err_I + err_R\n",
    "    return loss\n",
    "def train(self, epochs, ...):\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        self.optimizer.minimize(self.loss_fn, var_list=[self.betas])\n",
    "```\n",
    "* Training loop: at each step, this method simply performs a gradient update towards the minimization of the value returned by `loss_fn()` through `optimizer.minimize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "eps_R=1e-3\n",
    "gamma = 1./17\n",
    "\n",
    "baseline =  SIRModel(S, I, R, N, gamma=gamma, eps_R=eps_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [05:57<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* Let's firstly inspect the loss variation\n",
    "* The loss decreases quite regularly\n",
    "  * Apart from a small bump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d091aaae8de43beacf117c477aa6139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_history(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* Let's see the predictions for $\\beta$\n",
    "* The prediction is **very noisy**\n",
    "* However, a trend is clearly visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e76ff558f874af7b38b8ff5e17b100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pars(baseline, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* Let's run the simulation to obtain SIR data from fitted parameters\n",
    "* Actually, we are interested only in the $I$ compartment, since it raises the major issues\n",
    "* Running the SIR simulation actually produces decent results, not far from real data\n",
    "  * Employing both Euler's method (better) and RK4\n",
    "* Still, we have a jagged behaviour on $I$\n",
    "* Besides, we have no trustworthy estimation of $\\beta$\n",
    "  * We would like to have a clearer, *smoother* behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beb7f16a814460885ee30f39e7ca49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores: ['0.99 (E)', '0.98 (RK)']\n",
      "mse:['5.43e-04 (E)', '1.47e-03 (RK)']\n"
     ]
    }
   ],
   "source": [
    "plot_SIR(baseline, I, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An improvement: Semantic Based Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Smoothness through regularization<br/><br/>\n",
    "\n",
    "- To achieve meaningful and realistic results — and also to reasonably predict the evolution of the epidemic for short periods of time — we would like to enforce a smooth behaviour on $\\beta\\left(t\\right)$, rather than an irregular one which presents sudden changes.\n",
    "- Provided that we choose a tolerance threshold $\\Delta\\beta$, we can impose a constraint on the problem of the form\n",
    "\n",
    "$$\n",
    "\\mid\\beta_{t}-\\beta_{t-1}\\mid\\ \\leq\\ \\Delta\\beta \\ \\ \\ \\forall t \\in \\{1,...,T-1\\},\n",
    "$$\n",
    "\n",
    "- we can use the constraint to derive a **Semantic Based Regularizer** (SBR)\n",
    "  - even though it requires us to *tune the weight hyperparameter*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SBR model definition <br/><br/>\n",
    "\n",
    "Accordingly, we update the **loss function** for our model:\n",
    "$$\n",
    "f_{\\boldsymbol{\\beta}}\\left(I,R\\right) = \\frac{1}{T}\\sum_{t=0}^{T-1}{\\left(\\hat{I}_{t}-I_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2} + \\epsilon_{R}\\left(\\hat{R}_{t}-R_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2}} + \\lambda\\frac{1}{T-1}\\sum_{t=1}^{T-1}{\\left(\\beta_{t+1}-\\beta_{t}\\right)^{2}}\n",
    "$$\n",
    "where\n",
    "- the first term is the usual sum of the **Mean Squared Errors** on both the $I$ and the $R$ compartments\n",
    "- the second term is the **regularization** over $\\beta\\left(t\\right)$:\n",
    "  * the sum term represents the violation of the smoothness constraint\n",
    "  * the coefficient $\\lambda$ *weights* the violation of the smoothness constraint\n",
    "\n",
    "This loss function reflects our goals:\n",
    "* Obtain the **parameters $\\boldsymbol{\\beta}$** that **best fit the available data** ...\n",
    "* ... enforcing a trend for $\\beta\\left(t\\right)$ **as smooth as possible**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SBR model definition<br/><br/>\n",
    "\n",
    "* Let's implement the changes into the model\n",
    "\n",
    "```python\n",
    "def __init__(self, S, I, R, N, gamma=1./17, eps_R=1e-3, lambda_beta=0):\n",
    "    ...\n",
    "    # Parameters\n",
    "    self.initializer_beta = tf.keras.initializers.RandomUniform(minval=0, seed=42)\n",
    "    self.betas = tf.Variable(self.initializer_beta(shape=(self.T,), dtype='float64'), trainable=True)\n",
    "    # Weights\n",
    "    self.eps_R = tf.constant(eps_R, shape=(), dtype='float64')\n",
    "    self.lambda_beta = tf.Variable(lambda_beta, dtype='float64')\n",
    "    # Optimizer\n",
    "    self.optimizer = tf.keras.optimizers.Adam()\n",
    "    # Loss history\n",
    "    self.loss_history = tf.Variable([], dtype='float64')\n",
    "    self.mse_history = tf.Variable([], dtype='float64')\n",
    "    self.cst_history = tf.Variable([], dtype='float64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SBR model definition\n",
    "```python\n",
    "def loss_fn(self):\n",
    "    self.run_simulation()\n",
    "    # Error on I\n",
    "    err_I = K.mean(K.square(self.I_ - self.I))\n",
    "    # Error on R\n",
    "    err_R = self.eps_R * K.mean(K.square(self.R_ - self.R))\n",
    "    mse = err_I + err_R\n",
    "    # Regularization on beta\n",
    "    self.beta_cst = K.mean(K.square(self.betas[:-1] - self.betas[1:]))\n",
    "    cst = self.lambda_beta * self.beta_cst\n",
    "    loss = mse + cst\n",
    "    # Update loss history\n",
    "    self.loss_history = tf.concat([self.loss_history, [loss]], 0)\n",
    "    self.mse_history = tf.concat([self.mse_history, [mse]], 0)\n",
    "    self.cst_history = tf.concat([self.cst_history, [cst]], 0)\n",
    "    return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the SBR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "eps_R=1e-3\n",
    "gamma = 1./17\n",
    "\n",
    "sbr =  SIRModel(S, I, R, N, gamma=gamma, eps_R=eps_R, lambda_beta=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [05:57<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "sbr.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* The loss function decreases, as before\n",
    "  * However, we reach a greater absolute value with respect to the baseline\n",
    "  * Due to the *conflicting goals* of the two terms\n",
    "* We can notice the relevance of the constraint on the total loss\n",
    "  * Seems quite negligible, at least by inspecting the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a179fee68df477abb8fc66926961d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_history(sbr, reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* **Much of the noise** is gone, although some \"ups and downs\" still persist\n",
    "* This is the clearest evidence that the regularization is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdff4017139248adab54e06db92a6540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pars(sbr, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* The SIR simulation produces better results, in particular we obtain a more regular trend\n",
    "  * Once again, employing both Euler's method (still better) and RK4\n",
    "* However, we needed to tune another hyperparameter\n",
    "  * To find the right trade-off between MSE and regularization\n",
    "* Besides, the resulting prediction still presents irregularities\n",
    "  * It tends to stay close to true data, at the expense of smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60489448d7bc4ffa815d5e028dfbb2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores: ['0.99 (E)', '0.98 (RK)']\n",
      "mse:['1.19e-03 (E)', '1.69e-03 (RK)']\n"
     ]
    }
   ],
   "source": [
    "plot_SIR(sbr, I, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A recent approach: Lagrangian Duality for Constrained Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Smoothness through regularization... again!<br/><br/>\n",
    "<blockquote>\n",
    "<h3> Smoothness through regularization<br/><br/></h3>\n",
    "    \n",
    "- To achieve meaningful and realistic results — and also to reasonably predict the evolution of the epidemic for short periods of time — we would like to enforce a smooth behaviour on $\\beta\\left(t\\right)$, rather than an irregular one which presents sudden changes.\n",
    "- Provided that we choose a tolerance threshold $\\Delta\\beta$, we can impose a constraint on the problem of the form\n",
    "    \n",
    "$$\n",
    "\\mid\\beta_{t}-\\beta_{t-1}\\mid\\ \\leq\\ \\Delta\\beta \\ \\ \\ \\forall t \\in \\{1,...,T-1\\},\n",
    "$$\n",
    "    \n",
    "    \n",
    "- we could use the constraint to derive a **Semantic Based Regularizer** (SBR)\n",
    "  - even though it would require us to *tune the weight hyperparameter*\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "- or we could exploit a [recent approach](https://arxiv.org/pdf/2001.09394.pdf) to address this problem in a more robust and simplified fashion, as discussed in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lagrangian Duality for Constrained Learning <br/><br/>\n",
    "\n",
    "As recalled in the paper, when dealing with an optimization problem of the form\n",
    "$$\n",
    "\\cal O = \\underset{y}{\\operatorname{argmin}} f\\left(y\\right) \\ \\mathrm{\\text{ subject to }}\\ g_{i}\\left(y\\right)\\leq0 \\ \\left(\\forall i \\in \\left[n\\right]\\right),\n",
    "$$\n",
    "the use of (non-negative) *Lagrangian multipliers* allows to rewrite the objective function in its *Lagrangian relaxed form*, which ultimately **translates the constraints into loss penalties** for violating them.\n",
    "As the paper points out, the original problem then becomes\n",
    "$$\n",
    "f_{\\boldsymbol{\\lambda}}\\left(y\\right) = f\\left(y\\right) + \\sum_{i=1}^{n}{\\lambda_{i}g_{i}\\left(y\\right)} \\\\\n",
    "$$\n",
    "where $\\boldsymbol{\\lambda} = \\left(\\lambda_{1}, ..., \\lambda_{n}\\right)$ is the vector of the multipliers, with $\\lambda_{i}\\geq0$, while $g_{i}\\left(y\\right)$ are the penalty constraints (also expressed as $\\max\\left(0,g_{i}\\left(y\\right)\\right)$ in some formulations, to express the *non-negative* quantification of the violation).\n",
    "The problem amounts now to the optimization of the Lagrangian relaxation:\n",
    "$$\n",
    "LR_{\\boldsymbol{\\lambda}} = \\underset{y}{\\operatorname{argmin}} f_{\\boldsymbol{\\lambda}}\\left(y\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lagrangian Duality for Constrained Learning <br/><br/>\n",
    "\n",
    "Keeeping in mind that the relaxation is by construction a lower bound for the original problem:\n",
    "$$\n",
    "f\\left(LR_{\\boldsymbol{\\lambda}}\\right) \\leq f\\left(\\cal O\\right),\n",
    "$$\n",
    "we can consider an upper bound for it to get the *strongest relaxation* of the problem:\n",
    "\n",
    "$$\n",
    "LD = \\underset{\\lambda_{i} \\geq 0}{\\operatorname{argmax}} f\\left(LR_{\\boldsymbol{\\lambda}}\\right).\n",
    "$$\n",
    "\n",
    "- This latter problem is known as **Lagrangian dual** and it can be used to find the best *Lagrangian multipliers*.\n",
    "- The power of this method relies in turning the original problem into an *easier one*, at the same time enforcing a *strong approximation* of it.\n",
    "- Besides, it provides a further benefit with respect to the SBR approach:\n",
    "  - Within an SBR approach, the weight of the regularization is a *hyperparameter* to tune\n",
    "  - Within a Lagrangian dual approach, the weight (multiplier) is a *trainable variable*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lagrangian Duality: implementation <br/><br/>\n",
    "\n",
    "- Intuitively, we need a loop that iteratively seeks to first **minimize** the Lagrangian obtaining a class of *candidate lower bounds* for it, and then looks for the *greatest* candidate as the **tightest lower bound**.\n",
    "- The formal definition of the problem can be thus approximately translated into a loop which, for each optimization step $k$:\n",
    "  - Firstly performs a gradient step over $y$ so as to *minimize* the relaxed Lagrangian $f_{\\boldsymbol{\\lambda}}\\left(y\\right)$, obtaining the value $y_{\\boldsymbol{\\lambda}, k}$;\n",
    "  - Then performs another gradient step over each $\\lambda_{i}$ so as to *maximize* $f\\left(y_{\\boldsymbol{\\lambda}, k}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LD model definition <br/><br/>\n",
    "\n",
    "Recalling the **loss function** for our model:\n",
    "$$\n",
    "f_{\\boldsymbol{\\beta}}\\left(I,R\\right) = \\frac{1}{T} \\left(\\sum_{t=0}^{T-1}{\\left(\\hat{I}_{t}-I_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2} + \\epsilon_{R}\\left(\\hat{R}_{t}-R_{t}\\left(\\boldsymbol{\\beta}\\right)\\right)^{2}}\\right) + \\lambda_{\\boldsymbol{\\beta}}\\frac{1}{T-1}\\left(\\sum_{t=1}^{T-1}{\\left(\\beta_{t+1}-\\beta_{t}\\right)^{2}}\\right),\n",
    "$$\n",
    "\n",
    "Recalling the discussion about *Lagrangian Duality:*\n",
    "- $\\boldsymbol{\\beta}$ is our $y$, while our (only) <b>Lagrangian multiplier</b> is $\\lambda_{\\boldsymbol{\\beta}}$ (now a *trainable* parameter!)\n",
    "- we would like to firstly **_minimize the loss function_** over $\\boldsymbol{\\beta}$\n",
    "    - to do this, we take both MSE and regularization into account\n",
    "- then, exploiting Lagrangian Duality, we would like to **_maximize the loss function_** over the Lagrangian multiplier $\\lambda_{\\boldsymbol{\\beta}}$\n",
    "    - to do this, we update the Lagrangian multiplier at each step according to a Lagrangian step size $\\mu_{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LD model definition<br/><br/>\n",
    "\n",
    "* Let's apply the changes to our model accordingly:\n",
    "\n",
    "```python\n",
    "def __init__(self, S, I, R, N, gamma=1./17, eps_R=1e-3, lambda_beta=0, mu_beta=0):\n",
    "    ...\n",
    "    # Parameters\n",
    "    self.initializer_beta = tf.keras.initializers.RandomUniform(minval=0, seed=42)\n",
    "    self.betas = tf.Variable(self.initializer_beta(shape=(self.T,), dtype='float64'), trainable=True)\n",
    "    ...\n",
    "    # Lagrangian dual parameters\n",
    "    self.mu_beta = mu_beta\n",
    "    self.lambda_beta = tf.Variable(lambda_beta, dtype='float64')\n",
    "    self.beta_cst = tf.Variable(0, dtype='float64')\n",
    "    # Weight\n",
    "    self.eps_R = tf.constant(eps_R, shape=(), dtype='float64')\n",
    "    # Optimizer\n",
    "    self.optimizer = tf.keras.optimizers.Adam()\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LD model definition\n",
    "```python\n",
    "def optimize_lagrangian_parameters(self):\n",
    "    self.lambda_beta = self.lambda_beta + self.mu_beta * self.beta_cst\n",
    "def train(self, epochs, ldf=True):\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        self.optimizer.minimize(self.loss_fn, var_list=[self.betas])\n",
    "        if ldf:\n",
    "            self.optimize_lagrangian_parameters()\n",
    "```\n",
    "* Training loop: at each step, this method firstly performs a gradient update towards the minimization of the value returned by `loss_fn()` through `optimizer.minimize`, then another (custom) one towards the maximization of $\\lambda_{\\beta}$, performed as follows:\n",
    "$$\n",
    "\\lambda_{\\beta_{k+1}} = \\lambda_{\\beta_{k}} + \\mu_{\\beta_{k}}\\frac{1}{N-1}\\sum_{t=1}^{N-1}{\\left(\\beta_{k_{t+1}}-\\beta_{k_{t}}\\right)^{2}}\n",
    "$$\n",
    "where the sum quantifies the violation of the **smoothness constraint**, while the Lagrangian step size $\\mu_{\\beta}$ acts as a *learning rate coefficient* for the sum term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```python\n",
    "def predict(self, days):\n",
    "    self.run_simulation()\n",
    "    S = [self.S_[-1].numpy()]\n",
    "    I = [self.I_[-1].numpy()]\n",
    "    R = [self.R_[-1].numpy()]\n",
    "    N = self.N.numpy()\n",
    "    \n",
    "    # Compute beta: exponentially smoothed average of last 5 days\n",
    "    beta = self.betas[-5:]\n",
    "    beta_mean = tf.reduce_mean(beta)\n",
    "    soft = tf.nn.softmax(tf.linspace(0., len(beta)-1, len(beta))).numpy()\n",
    "    beta_avg = tf.reduce_mean((beta-beta_mean)*soft +beta_mean)\n",
    "    \n",
    "    gamma = self.gammas[0].numpy()\n",
    "    for day in range(days):\n",
    "        S_to_I = (beta_avg * I[-1] * S[-1]) / N\n",
    "        I_to_R = I[-1] * gamma\n",
    "        S.append(S[-1] - S_to_I)\n",
    "        I.append(I[-1] + S_to_I - I_to_R)\n",
    "        R.append(R[-1] + I_to_R)\n",
    "\n",
    "        return S, I, R, beta_avg.numpy()\n",
    "```\n",
    "\n",
    "* Employ the last values obtained by the simulation ($S_{T-1}$, $I_{T-1}$, $R_{T-1}$<sup>4</sup>) as initial point to run a simulation of `days` steps; the same parameter $\\beta_{p}$ was used for every step, obtained as  as follows:\n",
    "$$\n",
    "\\beta_{p} = \\frac{1}{n}\\sum_{t=0}^{n-1}{\\left(\\beta_{T-t-1}-\\bar{\\beta}\\right)\\sigma\\left(t\\right)+\\bar{\\beta}}\\ \\ \\ \\ \\ \\ \\mathrm{\\text with}\\ \\ \\sigma\\left(t\\right) = \\frac{e^{t}}{\\sum_{t=0}^{n-1}{e^{t}}}\n",
    "$$\n",
    "where we used the *softmax function* $\\sigma\\left(t\\right)$ and assigned $n=5$. This is a very simple method to obtain a rough approximation for $\\beta_{p}$: it basically computes an exponentially-smoothed weighted average of the last $n$ known values of $\\beta_{t}$, assigning a gradually *lower relevance to older values*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the LD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "eps_R=1e-3\n",
    "mu_beta=1e16\n",
    "gamma = 1./17\n",
    "\n",
    "lgd = SIRModel(S, I, R, N, gamma=gamma, eps_R=eps_R, lambda_beta=0, mu_beta=mu_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the LD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "lgd.train(epochs, ldf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inspecting results<br/><br/>\n",
    "\n",
    "* We sacrificed another (small) piece of loss in favour of the regularization\n",
    "* Again, the regularization loss accounts for an (apparently) negligible part of the main loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f7c98ddb9d411aa35f24b822653180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_history(lgd, reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitted parameters<br/><br/>\n",
    "\n",
    "* Take a look at the fitted parameters to evaluate the regularization\n",
    "* We can notice a **very smooth** behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288f000439dd4b14b07a874c42731ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pars(lgd, start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitted SIR<br/><br/>\n",
    "\n",
    "* As a result, we can enjoy a good fit\n",
    "    * Both accurate and smooth\n",
    "    * Euler's method still tends to perform better than RK4\n",
    "* We can notice a (slight) increase in the $MSE$ with respect to the SBR model\n",
    "  * The LD model ended up *sacrificing* another piece of prediction quality for the sake of smoother parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b9f5916af84d5082a6ae15220c58b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores: ['0.99 (E)', '0.98 (RK)']\n",
      "mse:['1.31e-03 (E)', '1.73e-03 (RK)']\n"
     ]
    }
   ],
   "source": [
    "plot_SIR(lgd, I, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#save_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conclusions<br/><br/>\n",
    "\n",
    "* We combined a model-driven approach based on ODEs and a data-driven approach based on Machine Learning\n",
    "* We employed Lagrangian Duality to **inject constraints** into the model\n",
    "* We **fitted** COVID-19 data to obtain the **parameters** of the epidemic\n",
    "* We ended up with a **descriptive model** for estimating the parameters of the epidemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Future work<br/><br/>\n",
    "\n",
    "* Now, if we have NPI (Non Pharmaceutical Interventions) data, it is possible to exploit our results to build a **predictive model**:\n",
    "  - Whose _input features_ are the NPIs\n",
    "  - Whose _target_ is fitted data ($\\beta$ parameters)\n",
    "* If we can manage to model the relationship between NPIs and $\\beta$, we could **predict** the evolution of the epidemic (in particular, of the infected)\n",
    "* It would then be possible to build a **prescriptive model**, namely a model to establish the best NPI to take in order to obtain a certain evolution of the epidemic.\n",
    "  - In particular, this could be achieved exploiting the [**Empirical Model Learning**](https://emlopt.github.io/) approach\n",
    "  - Namely, injecting the ML model into a *Combinatorial optimization model*...\n",
    "* ... but that is matter for another work!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
